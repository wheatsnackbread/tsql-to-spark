{"cells":[{"cell_type":"markdown","metadata":{},"source":["# SQL to Spark SQL Translator for Azure Databricks\n","\n","This notebook assists in translating T-SQL stored procedures to Spark SQL for use in Azure Databricks. It validates input, identifies dependencies, and generates Spark SQL code."]},{"cell_type":"markdown","metadata":{},"source":["\n","## Step 0: Prerequisites\n","- Ensure you have access to an Azure OpenAI subscription with a gpt-4o model provisioned. Increase the token per minute limit (TPM) to at least 130K. \n","- Duplicate the .env.sample file and name it .env before populating it.\n","- Install necessary libraries: (you can also run `pip install -r requirements.txt` in the terminal instead of this cell)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install required packages\n","%pip install pandas sqlglot openai networkx matplotlib python-dotenv tiktoken"]},{"cell_type":"markdown","metadata":{},"source":["- Load the dependencies and define common functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load libraries and environment variables\n","import os\n","import pandas\n","import networkx\n","import matplotlib\n","from IPython.display import Markdown, display\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# Define GPT-4o query function\n","from openai import AzureOpenAI\n","\n","client = AzureOpenAI(\n","    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n","    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n","    api_version = os.getenv(\"AZURE_OPENAI_PREVIEW_API_VERSION\"),\n",")\n","\n","temperature = float(os.getenv(\"AZURE_OPENAI_TEMPERATURE\"))\n","max_tokens = os.getenv(\"AZURE_OPENAI_MAX_TOKENS\")\n","\n","def gpt_4o_analysis(system_message, user_query):\n","    response = client.chat.completions.create(\n","        model=os.getenv(\"AZURE_OPENAI_MODEL_NAME\"),\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_message},\n","            {\"role\": \"user\", \"content\": f\"The user's query is: {user_query}\"}\n","        ],\n","        temperature=temperature\n","        # max_tokens=max_tokens\n","    )\n","    return response.choices[0].message.content.strip()\n","\n","print(\"Loaded libraries, environment variables, and defined GPT-4o query function.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Provide T-SQL Stored Procedure and Select Run Options\n","\n","Please paste your T-SQL stored procedure in the input cell below, or set read_from_file to true and populate bulk.sql:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set this variable to true to read from the file:\n","read_from_file = True\n","\n","if read_from_file:\n","\t# Import a file containing SQL and read it into tsql_procedure; each statement must be separated by a semicolon\n","\twith open('bulk.sql', 'r') as file:\n","\t\tbulk_sql = file.read()\n","\n","\t# Split ddl.sql into a list of strings by semicolons\n","\tbulk_sql_list = bulk_sql.split(';')\n","\n","# User input for T-SQL stored procedure\n","tsql_procedure = \"\"\"\n","\n","\n","\n","/** Your T-SQL code goes here **/\n","\n","\n","\n","\n","\"\"\"\n","print(\"Successfully loaded T-SQL stored procedure.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Input Validation and Preprocessing\n","\n","The tool will now validate and preprocess your T-SQL syntax to mitigate syntax-induced errors that occur before conversion."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not read_from_file:\n","    # Evaluate the T-SQL syntax correctness\n","    system_message = \"You are an AI assistant that helps validate T-SQL syntax. Identify only extremely critical errors that will prevent the SQL from executing. The dialect is T-SQL.\"\n","    user_query = tsql_procedure # \"CREATE PROS dbo.TestTable (ID int, Name varchar(50));\" # Wrong example to test\n","    result = gpt_4o_analysis(system_message, \"Evaluate the following T-SQL query. Apply all rules of the T-SQL dialect. Do not reprint the entire corrected script. Show only extremely major errors. Miscapitalized or abbreviated but correct keywords are not errors. Be as concise as possible. If no errors are detected, simply state so. \\n\\nSQL Query: \\n\\n\" + user_query)\n","    display(Markdown(result))\n","\n","else:\n","    for query in bulk_sql_list:\n","        # Evaluate the T-SQL syntax correctness\n","        system_message = \"You are an AI assistant that helps validate T-SQL syntax. Identify only extremely critical errors that will prevent the SQL from executing. The dialect is T-SQL.\"\n","        user_query = query\n","        result = gpt_4o_analysis(system_message, \"Evaluate the following T-SQL query. Apply all rules of the T-SQL dialect. Do not reprint the entire corrected script. Show only extremely major errors. Miscapitalized or abbreviated but correct keywords are not errors. Be as concise as possible. If no errors are detected, simply state so. \\n\\nSQL Query: \\n\\n\" + user_query)\n","        # Display the result and the original query, formatted for readability\n","\n","        display(Markdown(f\"### Original Query: \\n\\n```sql\\n{user_query}\\n```\\n\\n\"))\n","        display(Markdown(result))\n","\n","        print(\"\\n\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Dependency Analysis\n","\n","The tool will now analyze dependencies such as tables, views, and other database objects."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dependency analysis function using GPT-4o\n","def analyze_dependencies(tsql):\n","    system_message = \"You are an AI assistant that helps identify dependencies in T-SQL scripts. Extract and list all tables, views, and other database objects that the script depends on.\"\n","    user_query = tsql\n","    result = gpt_4o_analysis(system_message, \"Analyze the following T-SQL query and list all dependencies (tables, views, and other database objects). Be as concise as possible. Return each dependency in a newline and provide no commentary text. \\n\\nSQL Query: \\n\\n\" + user_query)\n","    \n","    print(result)\n","\n","    # Parse the result to extract dependencies\n","    dependencies = []\n","    # for line in result.split('\\n'):\n","    #     dependencies.append(line.strip())\n","\n","    dependencies = list(filter(None, result.split(\"\\n\")))\n","    \n","    return dependencies\n","\n","if not read_from_file:\n","    dependencies = analyze_dependencies(tsql_procedure)\n","    print(\"Dependencies identified:\", dependencies)\n","\n","else:\n","    for query in bulk_sql_list:\n","        dependencies = analyze_dependencies(query)\n","        # Display the result and the original query, formatted for readability\n","        display(Markdown(f\"### Original Query: \\n\\n```sql\\n{query}\\n```\\n\\n\"))\n","        print(\"Dependencies identified:\", dependencies)\n","\n","        print(\"\\n\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Token and Length Limit Check\n","\n","The total **context window** (combined input and output tokens) for the Azure OpenAI GPT-4o model must not exceed 128K. We use tiktoken to check the total length of the input and multiply it by two for a rough estimate of whether the script is too long for this translator to handle."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tiktoken\n","\n","# Initialize tiktoken encoding\n","encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n","\n","if not read_from_file:\n","    # Encode the T-SQL procedure to get the token count\n","    tokens = encoding.encode(tsql_procedure)\n","    input_token_count = len(tokens)\n","\n","    # Estimate the total context window needed (input tokens * 2)\n","    estimated_total_context_window = input_token_count * 2\n","\n","    # Define the maximum context window for Azure OpenAI GPT-4o model\n","    max_context_window = 128000\n","\n","    # Check if the estimated context window is within the allowable limit\n","    if estimated_total_context_window <= max_context_window:\n","        print(\"The T-SQL procedure is within the allowable context window for translation.\")\n","    else:\n","        print(\"The T-SQL procedure exceeds the allowable context window for translation.\")\n","        print(f\"Estimated total context window: {estimated_total_context_window} tokens\")\n","\n","    print(f\"Input token count: {input_token_count}\")\n","\n","else:\n","    for query in bulk_sql_list:\n","        # Encode the T-SQL query to get the token count\n","        tokens = encoding.encode(query)\n","        input_token_count = len(tokens)\n","\n","        # Estimate the total context window needed (input tokens * 2)\n","        estimated_total_context_window = input_token_count * 2\n","\n","        # Define the maximum context window for Azure OpenAI GPT-4o model\n","        max_context_window = 128000\n","\n","        # Display the result and the original query, formatted for readability\n","        display(Markdown(f\"### Original Query: \\n\\n```sql\\n{query}\\n```\\n\\n\"))\n","\n","        # Check if the estimated context window is within the allowable limit\n","        if estimated_total_context_window <= max_context_window:\n","            print(\"The T-SQL query is within the allowable context window for translation.\")\n","        else:\n","            print(\"The T-SQL query exceeds the allowable context window for translation.\")\n","            print(f\"Estimated total context window: {estimated_total_context_window} tokens\")\n","\n","        print(f\"Input token count: {input_token_count}\")\n","\n","        print(\"\\n\\n\\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5/6: Translation to Spark SQL\n","\n","The tool will now translate the T-SQL stored procedure to Spark SQL."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","import os\n","\n","# Translation to Spark SQL using GPT-4o\n","def translate_to_spark_sql(tsql):\n","    system_message = \"You are an AI assistant that helps translate T-SQL to Spark SQL. The dialect is Spark SQL.\"\n","    user_query = tsql + \" ---------- \\n\\n You must ensure that all referenced schemas do not contain square brackets, and are true to their source of stg_dbo or NO PREFIX if dbo.\"\n","    result = gpt_4o_analysis(system_message, \"Translate the following T-SQL query to Spark SQL that will be executed in an Azure Databricks environment. Be as precise and accurate as possible. Provide no additional commentary, returning just the translated query. Remember, stored procedures don't exist in Databricks. \\n\\nT-SQL Query: \\n\\n\" + user_query)\n","    return result\n","\n","if not read_from_file:\n","    spark_sql_procedure = translate_to_spark_sql(tsql_procedure)\n","    print(\"Translated Spark SQL:\")\n","    display(Markdown(spark_sql_procedure))\n","\n","else:\n","    for query in bulk_sql_list:\n","        spark_sql_query = translate_to_spark_sql(query)\n","        # Display the result and the original query, formatted for readability\n","        display(Markdown(f\"### Original Query: \\n\\n```sql\\n{query}\\n```\\n\\n\"))\n","        print(\"Translated Spark SQL:\")\n","        display(Markdown(spark_sql_query))\n","\n","        # Write same format to a new .txt file with custom timestamp in the filename\n","        \n","        # Create the output folder if it doesn't exist\n","        output_folder = \"output_spark_sql\"\n","        os.makedirs(output_folder, exist_ok=True)\n","        \n","        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","        file_name = f\"spark_sql_{timestamp}.txt\"\n","        file_path = os.path.join(output_folder, file_name)\n","        \n","        with open(file_path, \"w\") as text_file:\n","            text_file.write(\"### ORIGINAL QUERY \\n\\n\\n\"\"\" + query + \"\\n\\n\" + \"### TRANSLATED QUERY \\n\\n\\n\" + spark_sql_query)\n","\n","        print(f\"File '{file_name}' created in the '{output_folder}' folder.\")\n","\n","\n","        print('\\n\\n\\n')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<br><br><br>"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5/7: Translation to PySpark\n","\n","The tool will now translate the T-SQL stored procedure to PySpark."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","import os\n","\n","# Translation to PySpark using GPT-4o\n","def translate_to_pyspark(tsql):\n","    system_message = \"You are an AI assistant that helps translate T-SQL to PySpark.\"\n","### NOTE: IF YOU NEED TO REGENERATE WITH SPECIAL REQUESTS (i.e. bug fixes), SPECIFY CHANGES HERE:\n","    user_query = \"\"\"\n","     Assume that dataframes have NOT yet been instantiated for mentioned tables.\n","     For all tables preceded by \"dbo.\", assume they are in the default database and remove the prefix.\n","     This is because the tables have been migrated to the default database in Azure Databricks.\n","     You must ensure that all referenced schemas do not contain square brackets, and are true to their source of stg_dbo or NO PREFIX if dbo.\n","     Note that case sensitivity matters for PySpark table references.\n","    \"\"\"\n","\n","    result = gpt_4o_analysis(system_message, \"Your goal is to translate  the following T-SQL query to PySpark that will be executed in Azure Databricks environment. You don't need to configure or initialize a Spark context or Spark session, as these are managed for you by Azure Databricks. Be as precise and accurate as possible. Provide no additional commentary, returning just the translated query. \" + user_query + \"\\n\\nT-SQL Query: \\n\\n\" + tsql)\n","    return result\n","\n","if not read_from_file:\n","    pyspark_procedure = translate_to_pyspark(tsql_procedure)\n","    print(\"Translated PySpark:\")\n","    display(Markdown(pyspark_procedure))\n","\n","else:\n","    for query in bulk_sql_list:\n","        pyspark_procedure = translate_to_pyspark(query)\n","        # Display the result and the original query, formatted for readability\n","        display(Markdown(f\"### Original Query: \\n\\n```sql\\n{query}\\n```\\n\\n\"))\n","        print(\"Translated PySpark:\")\n","        display(Markdown(pyspark_procedure))\n","\n","        # Write same format to a new .txt file with custom timestamp in the filename\n","        \n","        # Create the output folder if it doesn't exist\n","        output_folder = \"output_pyspark\"\n","        os.makedirs(output_folder, exist_ok=True)\n","        \n","        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","        file_name = f\"pyspark_{timestamp}.txt\"\n","        file_path = os.path.join(output_folder, file_name)\n","        \n","        with open(file_path, \"w\") as text_file:\n","            text_file.write(\"### ORIGINAL QUERY \\n\\n\\n\"\"\" + query + \"\\n\\n\" + \"### TRANSLATED PYSPARK \\n\\n\\n\" + pyspark_procedure)\n","\n","        print(f\"File '{file_name}' created in the '{output_folder}' folder.\")\n","\n","        print(\"\\n\\n\\n\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
